{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from langdetect import detect_langs\n",
    "import pycld2 as cld2\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logFormatter = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(format=logFormatter, level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TG_DATA = \"./Data/TG_Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_between_html_tag(file_ptr,tag):\n",
    "        regex_t = re.compile(f\"(?<=<{tag}>)(.*)(?=</{tag}>)\",re.MULTILINE)\n",
    "\n",
    "        regex_t_s = re.compile(f\"<{tag}>\")\n",
    "        regex_t_e = re.compile(f\"</{tag}>\")\n",
    "        \n",
    "        regex_html_strip = re.compile(r'<.*?>')\n",
    "        \n",
    "        for line in file_ptr:\n",
    "            if regex_t_s.search(line):\n",
    "                match_t = regex_t.search(line)\n",
    "                if match_t:\n",
    "                    s = regex_html_strip.sub('',match_t.group())\n",
    "                    return tag,s\n",
    "                else:\n",
    "                    for newline in file_ptr:\n",
    "                        line = line + newline\n",
    "                        if regex_t_e.search(newline):\n",
    "                            match_t = regex_t.search(line.replace('\\n',' '))\n",
    "                            s = regex_html_strip.sub('',match_t.group().strip())\n",
    "                            return tag,s\n",
    "                        \n",
    "        return None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_headline(file_path):\n",
    "    fname = os.path.basename(file_path)\n",
    "    \n",
    "    with open(file_path,'r') as file_ptr:\n",
    "        html_sec,text = find_between_html_tag(file_ptr,'h1')\n",
    "        \n",
    "        # Separate loop for <p> ONLY IF <h> was found\n",
    "        if html_sec is None:\n",
    "            html_sec,text = find_between_html_tag(file_ptr,'p')\n",
    "    return fname,html_sec,text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filelist(folder_path = \".\"):\n",
    "    r_path = os.path.join(folder_path, \"**/*.html\")\n",
    "    file_list = [f for f in glob.glob(r_path, recursive=True)]\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the files\n",
    "\n",
    "#### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.03 s, sys: 2.54 s, total: 4.57 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_list = read_filelist(TG_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 20:33:35,641 - INFO - Number of files : 766886\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Number of files : {len(file_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 850 ms, sys: 396 ms, total: 1.25 s\n",
      "Wall time: 34.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = Pool()\n",
    "results= pool.map(find_headline, file_list)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 343 ms, sys: 19.9 ms, total: 363 ms\n",
      "Wall time: 370 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_headline = pd.DataFrame(results, columns=['fname', 'html_sec', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language tagging using `cld2` package\n",
    "\n",
    "##### `cld2` seem to give better results than `langdetect` package\n",
    "##### https://github.com/CLD2Owners/cld2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lang_prob(t):\n",
    "    top_l = None\n",
    "    top_l_prob = 0.0\n",
    "    \n",
    "    en_prob = 0.0\n",
    "    ru_prob = 0.0\n",
    "    try: \n",
    "        for l in t[2]:\n",
    "            if l[2]>top_l_prob:\n",
    "                top_l_prob = l[2]\n",
    "                top_l = l[1]\n",
    "            if l[1] == 'en':\n",
    "                en_prob = l[2]\n",
    "            elif l[1] == 'ru':\n",
    "                ru_prob = l[2]\n",
    "    except :\n",
    "        pass\n",
    "\n",
    "    return {'top_l' : top_l, 'top_l_prob' :top_l_prob ,'en_prob' : en_prob, 'ru_prob' : ru_prob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_langage(text,method = 'cld2'):\n",
    "    # Pass the 'method' parameter for deferent\n",
    "    # models. \n",
    "    # Valid params = [cld2,langdetect,polyglot]\n",
    "    \n",
    "    ## Encode to utf-8\n",
    "    text = text.encode('utf-8').decode(\"utf-8\", \"ignore\")\n",
    "    \n",
    "    try:\n",
    "        if method == 'cld2':\n",
    "            # Pass to cld2\n",
    "            result = cld2.detect(text, bestEffort=False)\n",
    "        elif method == 'langdetect':\n",
    "            ### TODO : return values properly\n",
    "            result = detect_langs(text)\n",
    "        elif method == 'polyglot':\n",
    "            ### TODO : implement polyglot\n",
    "            result = tuple()\n",
    "        else:\n",
    "            result = tuple()\n",
    "    except:\n",
    "        result = tuple()\n",
    "    \n",
    "    # Now, compute the probabilities\n",
    "    _p = compute_lang_prob(result)\n",
    "    return _p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 459 ms, total: 1.52 s\n",
      "Wall time: 2.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = Pool()\n",
    "results= pool.map(detect_langage, list(df_headline['text']))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probs = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Compute result and update it to a df\n",
    "# df_probs = pd.DataFrame(list(df_headline['text'].apply(lambda x : detect_langage(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.concat([df_headline,df_probs],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>html_sec</th>\n",
       "      <th>text</th>\n",
       "      <th>top_l</th>\n",
       "      <th>top_l_prob</th>\n",
       "      <th>en_prob</th>\n",
       "      <th>ru_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4120487137627119965.html</td>\n",
       "      <td>h1</td>\n",
       "      <td>中国冀望加强与印尼的渔业合作</td>\n",
       "      <td>zh</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2053985518435310483.html</td>\n",
       "      <td>h1</td>\n",
       "      <td>تكريم مصطفى قمر عن مجمل أعماله بقصر السينما.. صور</td>\n",
       "      <td>ar</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7679282538646280504.html</td>\n",
       "      <td>h1</td>\n",
       "      <td>बैंकों के विलय से 7000 शाखाओं पर खतरा, आप पर ह...</td>\n",
       "      <td>hi</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1105816786000706407.html</td>\n",
       "      <td>h1</td>\n",
       "      <td>Punjab as a driver for Pakistan’s growth</td>\n",
       "      <td>en</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6181833003973924891.html</td>\n",
       "      <td>h1</td>\n",
       "      <td>Thánh lễ ở London cầu nguyện cho 39 nạn nhân c...</td>\n",
       "      <td>vi</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      fname html_sec  \\\n",
       "0  4120487137627119965.html       h1   \n",
       "1  2053985518435310483.html       h1   \n",
       "2  7679282538646280504.html       h1   \n",
       "3  1105816786000706407.html       h1   \n",
       "4  6181833003973924891.html       h1   \n",
       "\n",
       "                                                text top_l  top_l_prob  \\\n",
       "0                                     中国冀望加强与印尼的渔业合作    zh        97.0   \n",
       "1  تكريم مصطفى قمر عن مجمل أعماله بقصر السينما.. صور    ar        98.0   \n",
       "2  बैंकों के विलय से 7000 शाखाओं पर खतरा, आप पर ह...    hi        99.0   \n",
       "3           Punjab as a driver for Pakistan’s growth    en        97.0   \n",
       "4  Thánh lễ ở London cầu nguyện cho 39 nạn nhân c...    vi        98.0   \n",
       "\n",
       "   en_prob  ru_prob  \n",
       "0      0.0      0.0  \n",
       "1      0.0      0.0  \n",
       "2      0.0      0.0  \n",
       "3     97.0      0.0  \n",
       "4      0.0      0.0  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect results obtained\n",
    "\n",
    "There seem to be quite a few Fale negatives (Model tagged the article as None\n",
    "1. We can run other taggers on this and take output\n",
    "2. We can fetch the body of these files and then tag using that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(df_result[(df_result['top_l']=='en') & (df_result['top_l_prob'] < 99)]['text']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(df_result[(df_result['top_l']=='en') & (df_result['top_l_prob'] > 90) &\n",
    "#                    (df_result['top_l_prob'] < 96)]['text']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df_result[(df_result['top_l'].isna())]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result[df_result['fname'] == '1360813307990682336.html'].iloc[0,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the output\n",
    "\n",
    "Sample : \n",
    "\n",
    "    [\n",
    "      {\n",
    "        \"lang_code\": \"en\",\n",
    "        \"articles\": [\n",
    "          \"981787246124324.html\",\n",
    "          \"239748235923753.html\",\n",
    "          ...\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"lang_code\": \"ru\",\n",
    "        \"articles\": [\n",
    "          \"273612748127432.html\",\n",
    "          ...\n",
    "        ]\n",
    "      },\n",
    "      ...\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, extract the cases where model was > 95% sure\n",
    "en_articles = list(df_result[df_result['en_prob']>=95]['fname'])\n",
    "ru_articles = list(df_result[df_result['ru_prob']>=95]['fname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_output(lang_code,article_list):\n",
    "    #TODO : Make sure lang_code is a valid \n",
    "    #       ISO 639-1 two-letter language code\n",
    "    d = {\"lang_code\" : lang_code,\"articles\":article_list}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [prepare_output(\"en\",en_articles),prepare_output(\"ru\",ru_articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time : 12 seconds\n"
     ]
    }
   ],
   "source": [
    "# print(\"Total execution time : \" + \"{0:.2}\".format(str(time.time()-start_time)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating\n",
    "If we pass in the whole document and it's the language detectors can perform way better. We will test that out with the accuracy obtained by headline based language detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(file):\n",
    "    with open(file,'r') as file_ptr:\n",
    "        soup = BeautifulSoup(file_ptr,'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meta(soup):\n",
    "    d = {}\n",
    "    \n",
    "    #TODO : Add exception handle to all of this\n",
    "    try: \n",
    "        d['title'] = soup.find(\"meta\",  property=\"og:title\")['content']\n",
    "    except TypeError as e:\n",
    "        logger.error('Title not found')\n",
    "        d['title'] = \"\"\n",
    "    \n",
    "    try:\n",
    "        d['url'] = soup.find(\"meta\",  property=\"og:url\")['content']\n",
    "    except TypeError as e:\n",
    "        logger.error('Title not found')\n",
    "        d['url'] = \"\"\n",
    "    \n",
    "    try:\n",
    "        d['site_name'] = soup.find(\"meta\",  property=\"og:site_name\")['content']\n",
    "    except TypeError as e:\n",
    "        logger.error('Title not found')\n",
    "        d['site_name'] = \"\"\n",
    "    \n",
    "    try:\n",
    "        d['published_time'] = soup.find(\"meta\",  property=\"article:published_time\")['content']\n",
    "    except TypeError as e:\n",
    "        logger.error('Title not found')\n",
    "        d['published_time'] = \"\"\n",
    "    \n",
    "    try:\n",
    "        d['description'] = soup.find(\"meta\",  property=\"og:title\")['content']\n",
    "    except TypeError as e:\n",
    "        logger.error('Title not found')\n",
    "        d['published_time'] = \"\"\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(soup,tag = 'all'):\n",
    "    assert tag in ['all','p','h1']\n",
    "    if tag == 'all':\n",
    "        text = soup.text.strip()\n",
    "    else:\n",
    "        p_contents = soup.find_all(tag)\n",
    "        text = \"\"\n",
    "        for p in p_contents:\n",
    "            text = text + p.getText()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_file(file):\n",
    "    soup = get_soup(file)\n",
    "    d = extract_meta(soup)\n",
    "    d['p_text'] = extract_text(soup,'p')\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Make a good sanitization function @Jun\n",
    "def sanitize_text(text):\n",
    "    sane_text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '',text, flags=re.MULTILINE)\n",
    "    sane_text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '',sane_text, flags=re.MULTILINE)\n",
    "    sane_text = text\n",
    "    return sane_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 20:59:07,606 - ERROR - Title not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.33 s, sys: 7.7 s, total: 17 s\n",
      "Wall time: 5min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = Pool()\n",
    "results= pool.map(parse_html_file, file_list)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 20:59:38,434 - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "df_parsed = pd.DataFrame(results)\n",
    "df_parsed['all_text'] = df_parsed['title'] + \"\\n\" + df_parsed['p_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 s, sys: 18.4 s, total: 28.4 s\n",
      "Wall time: 47.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = Pool()\n",
    "results= pool.map(detect_langage, list(df_parsed['all_text']))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probs_a = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Compute result and update it to a df\n",
    "# df_probs_a = pd.DataFrame(list(df_parsed['all_text'].apply(lambda x : detect_langage(sanitize_text(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_a = pd.concat([df_parsed,df_probs_a],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2451,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_a[df_result_a.top_l.isna()]['all_text'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df_result_a[(df_result_a['top_l']=='en') & (df_result_a['top_l_prob'] < 90)][['title','top_l_prob']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([585318,   2824,    546,    226,    152,     65,     74,    120,\n",
       "           389, 177172]),\n",
       " array([ 0. ,  9.9, 19.8, 29.7, 39.6, 49.5, 59.4, 69.3, 79.2, 89.1, 99. ]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(df_result_a.en_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
