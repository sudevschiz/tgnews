{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pycld2 as cld2\n",
    "from multiprocessing import Pool\n",
    "import text_preprocessing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9c1091bef541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### LOGGER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlogFormatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%(asctime)s - %(levelname)s - %(message)s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasicConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogFormatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "### LOGGER\n",
    "logFormatter = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(format=logFormatter, level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filelist(folder_path,**kwargs):\n",
    "    '''Return the relative file path of all the html files \n",
    "    which are present in the path\n",
    "    \n",
    "    By defauly, the search is recursive and extends to all the folders\n",
    "    present in the folder_path provided.\n",
    "    \n",
    "    Pass `recursive=False` for fetch only the current\n",
    "    directory\n",
    "    \n",
    "    Pass the file extension desired as 'file_type'.\n",
    "    Eg: file_type = html (Default is 'htm')\n",
    "    \n",
    "    '''\n",
    "    try:\n",
    "        recursive = kwargs['recursive']\n",
    "        if not isinstance(recursive,bool):\n",
    "            logger.info('Recursive option not passed correctly. Defaulting to True')\n",
    "            recursive = True\n",
    "    except KeyError as e:\n",
    "        recursive = True\n",
    "    \n",
    "    try:\n",
    "        file_type = kwargs['file_type']\n",
    "        if not isinstance(file_type,str):\n",
    "            logger.info(\"Defaulting to 'htm'\")\n",
    "            file_type = 'htm'\n",
    "    except KeyError as e:\n",
    "        logger.info(\"Defaulting to 'htm*'\")\n",
    "        file_type = 'htm*'\n",
    "    r_path = os.path.join(folder_path, \"**/*.\"+file_type)\n",
    "    file_list = [f for f in glob.glob(r_path, recursive=recursive)]\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def get_soup(file):\n",
    "    '''Return the BeautifulSoup object of the html file provided'''\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    with open(file,'r') as file_ptr:\n",
    "        soup = BeautifulSoup(file_ptr,'lxml')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def extract_meta(soup):\n",
    "    '''This is a specific function for the type of file TG has\n",
    "    provided in the samples. The look-up values are based on that\n",
    "    \n",
    "    If those fields are not found (as in the case of a generic html)\n",
    "    empty values are returned in those cases'''\n",
    "    d = {}\n",
    "    \n",
    "    #TODO : Add exception handle to all of this\n",
    "    try: \n",
    "        d['title'] = soup.find(\"meta\",  property=\"og:title\")['content']\n",
    "    except TypeError as e:\n",
    "#         logger.error('Title not found')\n",
    "        d['title'] = \"\"\n",
    "    \n",
    "    try:\n",
    "        d['url'] = soup.find(\"meta\",  property=\"og:url\")['content']\n",
    "    except TypeError as e:\n",
    "#         logger.error('Title not found')\n",
    "        d['url'] = \"\"\n",
    "    \n",
    "    try:\n",
    "        d['site_name'] = soup.find(\"meta\",  property=\"og:site_name\")['content']\n",
    "    except TypeError as e:\n",
    "#         logger.error('Title not found')\n",
    "        d['site_name'] = \"\"\n",
    "    \n",
    "    try:\n",
    "        d['published_time'] = soup.find(\"meta\",  property=\"article:published_time\")['content']\n",
    "    except TypeError as e:\n",
    "#         logger.error('Title not found')\n",
    "        d['published_time'] = \"\"\n",
    "    \n",
    "    try:\n",
    "        d['description'] = soup.find(\"meta\",  property=\"og:title\")['content']\n",
    "    except TypeError as e:\n",
    "#         logger.error('Title not found')\n",
    "        d['published_time'] = \"\"\n",
    "    \n",
    "    return d\n",
    "\n",
    "def extract_text(soup,tag = 'all'):\n",
    "    '''Takes the soup objects and the tag name as inputs,\n",
    "    returns all the text in that tag concatenated \n",
    "    together'''\n",
    "    \n",
    "    if tag == 'all':\n",
    "        text = soup.text.strip()\n",
    "    else:\n",
    "        p_contents = soup.find_all(tag)\n",
    "        text = \"\"\n",
    "        for p in p_contents:\n",
    "            text = text + \" \" + p.getText()\n",
    "    return text\n",
    "\n",
    "\n",
    "def sanitize_text(text):\n",
    "    import re\n",
    "    sane_text = re.sub(r'^https?:\\\\/\\\\/.*[\\\\r\\\\n]*', '',text, flags=re.MULTILINE)\n",
    "    sane_text = bytes(sane_text, 'utf-8').decode('utf-8','ignore')\n",
    "    \n",
    "    return sane_text\n",
    "\n",
    "\n",
    "def extract_links(soup,domain=False):\n",
    "    '''Takes the soup objects and returns all the links'''\n",
    "    links = [a.get('href') for a in soup.find_all('a', href=True)]\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "def parse_html_file(file):\n",
    "    '''Uses bs4 to get the soup of the file and calls\n",
    "    the other extraction functions\n",
    "    TODO : Better html parsers are available'''\n",
    "    soup = get_soup(file)\n",
    "    d = extract_meta(soup)\n",
    "    d['p_text'] = extract_text(soup,'p')\n",
    "    d['links'] = extract_text(soup,'a')\n",
    "    d['all_text'] = d['title'] + \"\\n\" + d['p_text']\n",
    "    d['links'] =extract_links(soup)\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### FUNCTIONS FOR LANGUAGE DETECTION\n",
    "def compute_lang_prob(t):\n",
    "    '''Returns the top language code identified.\n",
    "    Also returns the probaility of English and Russian\n",
    "    (Specific to this problem statement)'''\n",
    "    top_l = None\n",
    "    top_l_prob = 0.0\n",
    "    \n",
    "    en_prob = 0.0\n",
    "    ru_prob = 0.0\n",
    "    try: \n",
    "        for l in t[2]:\n",
    "            if l[2]>top_l_prob:\n",
    "                top_l_prob = l[2]\n",
    "                top_l = l[1]\n",
    "            if l[1] == 'en':\n",
    "                en_prob = l[2]\n",
    "            elif l[1] == 'ru':\n",
    "                ru_prob = l[2]\n",
    "    except :\n",
    "        pass\n",
    "\n",
    "    return {'top_l' : top_l, 'top_l_prob' :top_l_prob ,'en_prob' : en_prob, 'ru_prob' : ru_prob}\n",
    "\n",
    "\n",
    "def detect_langage(text,method = 'cld2'):\n",
    "    '''For each piece of text input, this\n",
    "    function uses the method passed to return\n",
    "    the detected languages\n",
    "    Pass the 'method' parameter for different models. \n",
    "    Valid params = [cld2,langdetect,polyglot]'''\n",
    "    \n",
    "    ## Encode to utf-8\n",
    "    text = text.encode('utf-8').decode(\"utf-8\", \"ignore\")\n",
    "    \n",
    "    try:\n",
    "        if method == 'cld2':\n",
    "            # Pass to cld2\n",
    "            result = cld2.detect(text, bestEffort=False)\n",
    "        elif method == 'langdetect':\n",
    "            ### TODO : return values properly\n",
    "            result = detect_langs(text)\n",
    "        elif method == 'polyglot':\n",
    "            ### TODO : implement polyglot\n",
    "            result = tuple()\n",
    "        else:\n",
    "            result = tuple()\n",
    "    except Exception as e:\n",
    "#         logger.error(e)\n",
    "        result = tuple()\n",
    "    \n",
    "    # Now, compute the probabilities\n",
    "    _p = compute_lang_prob(result)\n",
    "    return _p\n",
    "\n",
    "\n",
    "def detect_distributed(file):\n",
    "    '''This function calls the process in order.\n",
    "    Parallizes well.'''\n",
    "#     soup = get_soup(file)\n",
    "#     d = extract_meta(soup)\n",
    "#     d['p_text'] = extract_text(soup,'p')\n",
    "#     d['all_text'] = sanitize_text(d['title'] + \"\\n\" + d['p_text'])\n",
    "    d = parse_html_file(file)\n",
    "    d.update(detect_langage(d['all_text']))\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "def label_final_lang(df_prob,prob=0.95):\n",
    "    '''Once the probabilities are calculated,\n",
    "    prepare the list of EN and RU articles.\n",
    "    TODO : Ideally this should scale to any language.'''\n",
    "    # For now, extract the cases where model was > 95% sure\n",
    "    en_articles = list(df_prob[df_prob['en_prob']>=prob]['fname'])\n",
    "    ru_articles = list(df_prob[df_prob['ru_prob']>=prob]['fname'])\n",
    "    return en_articles,ru_articles\n",
    "\n",
    "\n",
    "def prepare_output(lang_code,article_list):\n",
    "    '''Prepare the JSON output in the desired manner\n",
    "    TODO : Make sure lang_code is a valid ISO 639-1 \n",
    "    two-letter language code'''\n",
    "    \n",
    "    d = {\"lang_code\" : lang_code,\"articles\":article_list}\n",
    "    return d\n",
    "\n",
    "\n",
    "def languages(path,**kwargs):\n",
    "    '''This function outputs the EN and RU \n",
    "    articles in the provided path. \n",
    "    If full path of the files are required,\n",
    "    pass full_path = True'''\n",
    "    file_list = read_filelist(path)\n",
    "    logger.info(f'Number of files : {len(file_list)}')\n",
    "    \n",
    "    with Pool() as pool:\n",
    "        results = pool.map(detect_distributed, file_list)\n",
    "    \n",
    "    df_prob = pd.DataFrame(results)\n",
    "    df_prob['html_dict'] = list(results)\n",
    "    \n",
    "    try:\n",
    "        full_path = kwargs['full_path']\n",
    "        if full_path:\n",
    "            df_prob['fname'] = [f for f in file_list]\n",
    "        else:\n",
    "            df_prob['fname'] = [os.path.basename(f) for f in file_list]\n",
    "    except KeyError as e:\n",
    "        logger.warning('Returning only the file name')\n",
    "        df_prob['fname'] = [os.path.basename(f) for f in file_list]\n",
    "    \n",
    "\n",
    "    # For now, extract the cases where model was > 95% sure\n",
    "    try:\n",
    "        threshold = kwargs['threshold']\n",
    "    except KeyError as e:\n",
    "        logger.info(\"Setting default probabiltiy at 0.95\")\n",
    "        threshold = 0.95\n",
    "    \n",
    "    en_articles,ru_articles = label_final_lang(df_prob,prob=threshold)\n",
    "    \n",
    "    output = prepare_output(\"en\",en_articles),prepare_output(\"ru\",ru_articles)\n",
    "    \n",
    "    try:\n",
    "        return_parsed_df = kwargs['return_parsed_df']\n",
    "        if return_parsed_df:\n",
    "            return output,df_prob\n",
    "        else:\n",
    "            return output\n",
    "    except KeyError as e:\n",
    "        logger.info('Returning output dictionary only.')\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    import io\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    return n,d,data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out memory requirements for saving N number of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 44.3 s, total: 1min 56s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n,d,ft_dict = load_vectors(\"assets/wiki-news-300d-1M.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999994"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ft_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dict(itertools.islice(ft_dict.items(), 100000)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('assets/wiki_news_ft_300D_selected.json', 'w') as outfile:\n",
    "    json.dump(out, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takes aroung 253 MB to save 100K vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get only the English articles using the languages module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-02 23:07:57,815 - INFO - Defaulting to 'htm*'\n",
      "2019-12-02 23:08:03,731 - INFO - Number of files : 1080604\n",
      "2019-12-02 23:22:56,107 - INFO - Setting default probabiltiy at 0.95\n",
      "2019-12-02 23:22:58,560 - INFO - Returning output dictionary only.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 2min 29s, total: 3min 31s\n",
      "Wall time: 15min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "en_ru_articles = languages('Data/TG_Data/',full_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_articles = en_ru_articles[0]['articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(8) as pool:\n",
    "    results = pool.map(parse_html_file,en_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the top N words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "pbar = ProgressBar(maxval=len(results)).start()\n",
    "\n",
    "i = 0\n",
    "text = []\n",
    "for h in results:\n",
    "    text.append(h['all_text'])\n",
    "    \n",
    "    i+=1\n",
    "    pbar.update(i)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/all_text.json','w') as f:\n",
    "    json.dump(text,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/sudevchirappat/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/sudevchirappat/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"/Users/sudevchirappat/Documents/Projects/tgnews/text_preprocessing.py\", line 90, in preprocess\n    sample = replace_contractions(sample)\n  File \"/Users/sudevchirappat/Documents/Projects/tgnews/text_preprocessing.py\", line 11, in replace_contractions\n    return contractions.fix(text)\n  File \"/Users/sudevchirappat/opt/anaconda3/lib/python3.7/site-packages/contractions/__init__.py\", line 211, in fix\n    return ts.replace(s)\n  File \"/Users/sudevchirappat/opt/anaconda3/lib/python3.7/site-packages/textsearch/__init__.py\", line 561, in replace\n    start, stop, result = handler(text, start, stop, norm)\n  File \"/Users/sudevchirappat/opt/anaconda3/lib/python3.7/site-packages/textsearch/__init__.py\", line 371, in bounds_check\n    if len(text) != stop and text[stop] in self.right_bound_chars:\nIndexError: string index out of range\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with Pool(8) as pool:\n",
    "    results = pool.map(tp.preprocess, text)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
